<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>S15: 15-618 Final Project by hpandit and rbandlam</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <style>
    table, th, td {
        border: 1px solid white;
        border-collapse: collapse;
    }
    th, td {
        padding: 2px;
    }
    th,td  {
    text-align: center;
    } 
  </style>
  </head>
  <body>
    <div class="wrapper">
  
      <section>
        <div id="title">
          <h1>"CUDA Factor": A Library of Matrix Factorization routines  </h1>
          <p> Harshavardhan Pandit and Ravi Chandra Bandlamudi</p>
          <br>
          <br>
          <hr>
          <br>
          <br>
          <br>
          <br>
          <h2 style="color:SkyBlue">
          <a id="Index" class="bottom-three" href="#Index" aria-hidden="true"><span class="octicon octicon-link"></span></a><center><bottom-three><a href="#Writeup">Writeup</a> &nbsp; :: &nbsp;<a href="#Checkpoint">Checkpoint</a> &nbsp; :: &nbsp; <a href="#Proposal">Proposal</a></bottom-three></center></h2>
          
        </div>

<br>
<br>
<br>
<br>
<br>
<br>
<br>

<h1 style="color:Gold">
<a id="Writeup" class="anchor" href="#Writeup" aria-hidden="true"><span class="octicon octicon-link"></span></a><center>Writeup</center></h2>

<h1 style="color:Gold">
<a id="Checkpoint" class="anchor" href="#Checkpoint" aria-hidden="true"><span class="octicon octicon-link"></span></a><center>Checkpoint</center></h2>

<p> We have thus far made progress with LU and QR factorizations, as well as optimized matrix multiply 
    (which was one of the "nice to haves"). To begin with, a simple sequential algorithm was implemented
    to compute the LU factorization. We then implemented a naive CUDA version of the same and began optimizing
    it. The performance of the LU factorization depends heavily on the memory access patterns and two different
    approaches to accessing memory were tried out. The matrix was divided into blocks and row-wise and column-wise
    accesses within each block were tried out. Another optimization was to use shared memory within each block to
    store the data which is accesses by all threads within the block.</p>
    

<p> We faced some problems initially implementing the blocked version in CUDA, mainly related to array indexing 
    and memory acceses.  We also set up benchmarks to compare our routines to. The in-built
    CUBLAS (CUDA Basic Linear Algebra Subroutines) library is used as a reference. However, the shared memory version
    of LU factorization still has some bugs and fails for higher block sizes, an issue which we are still working on.
    For this reason, timing the LU routine was deferred. </p>
    
<p> The implementation of the QR factorization routine was started simultaneously, which is characterized not by 
    access patterns but by matrix-matrix and matrix-vector multiplications. Again, a sequential version was first implemeneted and then
    a naive CUDA one. With the naive CUDA version, our implementation is 2-3 times worse than the CUBLAS reference
    library for higher matrix dimensions.</p>  
    
  <center><table align="center" style="width:100%">
  <tr>
    <th>Matrix Size</th>
    <th>CUBLAS Time</th>
    <th>Our Time</th>
  </tr>
  <tr>
    <td>4096 x 1024</td>
    <td>47.88s</td>
    <td>155.41s</td>		
  </tr>
  <tr>
    <td>4096 x 2048</td>
    <td>174.01s</td>
    <td>340.02s</td>		
  </tr>
  </table></center>
  
  <p><center>Preliminary results</center></p>
    
<p> Because of the fact that QR decomposition has many matrix-matrix multiplications, it makes sense for us to
    focus on one of our "nice to haves" - the matrix multiplication routine - and optimize it at this stage. One
    of the optimizations which we think will improve our run time performance of the QR routine is to modify the
    algorithm slightly so that the memory allocations are optimal. Another major one would be to use optimized 
    matrix multiply routine instead of the naive one which we have currently written.</p>
    
<p> Finally, we believe that we would be able to produce a library with atleast 3 fast and efficient matrix 
    routines - LU factorization, Matrix Multiply and QR factorization. If all goes as planned, we think that the
    other "nice to have" - Cholesky decomposition - is also within reach for us, before the deadline. For the
    Parallelism Competition, we will display a comparison of our implementation with respect to the CUBLAS library
    in the form of a graph.</p>
    
<br>
<br>
<h1 style="color:Gold">
<a id="Proposal" class="anchor" href="#Proposal" aria-hidden="true"><span class="octicon octicon-link"></span></a><center>Proposal</center></h2>

<h2 style="color:white">
<a id="Summary" class="anchor" href="#Summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h2>

<p> We will be developing a library of GPU-accelerated optimized matrix factorization routines in CUDA.</p>


<h2 style="color:white">
<a id="Background" class="anchor" href="#Background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h2>

<p> The factorization routines focused on in this project are LU and QR (and, if time permits, Cholesky)
    decomposition. Both these function involve splitting a given matrix into a product of two matrices. </p>

<h4 style="color:white">
<a id="LU Decomposition" class="anchor" href="#LU" aria-hidden="true"><span class="octicon octicon-link"></span></a>LU Decomposition</h4>

<p> LU decomposition involves splitting a regular NxN matrix A into a product of a lower triangular matrix L
    and an upper triangular matrix U using Gaussian elimination. This factorization finds applications
    in simplifying linear algebra operations such solving a system of linear equations Ax = b, calculating
    the determinant of a matrix, inverting a matrix etc. </p>
    
<p> A sequential "right-looking" algorithm loops over columns and updates columns to its right based on
    the current column. The following pseudo-code describes the algorithm: </p>
    
 <code>
 <pre>
    for i=1 to N  /*loop over columns*/
      for j= i+1 to N
        A(j,i) = A(j,i)/A(i,i)  /*update L*/
        for k=i+1 to N  
          A(j,k) = A(j,k) - A(i,k)*A(j,i)  /*update U*/
        end
      end
    end  
  </pre>  
 </code>    
 
<h4 style="color:white">
<a id="QR Decomposition" class="anchor" href="#LU" aria-hidden="true"><span class="octicon octicon-link"></span></a>QR Decomposition</h4>
 
 <p>
   QR factorization splits a rectangular MxN matrix A into two matrices Q and R such that A=QR. Here, Q is a 
   MxM orthogonal matrix whereas R is a NxN upper triangular matrix. This decomposition too is very useful in finding
   solutions to a system of linear equations. QR decomposition is also widely used to solve the linear as well as
   the nonlinear least squares problem.
 </p>
 
 <p>
   The algorithm to decompose a matrix into QR factors is based on the so called Givens rotations which are
   orthogonal. Using a sequence of givens rotations the given matrix can be transformed to an upper 
   triangular matrix. Givens rotations can be systematically applied to successive pairs of rows of matrix A
   to zero entire strict lower triangle. Parallelizing the computation of Givens rotations is one avenue but 
   there could also exist an opportunity to parallelize the step of applying this transformation to pairs of rows.
 </p>
<!-- <img src="images/test_image.jpg" alt="W3Schools.com" width="104" height="142"> -->

<h2 style="color:white">
<a id="Challenge" class="anchor" href="#Challenge" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenge</h2>

<h4 style="color:white">
<a id="LU Decomposition" class="anchor" href="#LU" aria-hidden="true"><span class="octicon octicon-link"></span></a>LU Decomposition</h4>

<p>
  The sequential algorithm for the LU decomposition will have to be modified in order to efficiently parallelize 
  it. The data dependencies in the algorithm in outermost loop will have to be dealt with. The algorithm in its
  current form, does not have independent outer loop iterations. Also, the 3 different loops within a matrix imply
  that there are different access patterns possible and an efficient implementation will use one which best exploits locality.
  The challenging part would be to implement a parallel version of the algorithm which best takes advantage of the
  locality in the problem and efficiently uses GPU resources such as shared memory. Thus, the problem should be an
  interesting opportunity to apply platform-specific and application-specific optimizations.
</p>

<h4 style="color:white">
<a id="QR Decomposition" class="anchor" href="#LU" aria-hidden="true"><span class="octicon octicon-link"></span></a>QR Decomposition</h4>

<p>
  The challenge in this routine is to come up with an algorithm which reduces the amount of synchronization needed. 
  The idea is to exploit parallelism in as many steps of the algorithm as we can by efficiently implementing it on
  a GPU.
</p>

<h2 style="color:white">
<a id="Resources" class="anchor" href="#Resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resources</h2>

<p>We will be using a GPU on one of the GHC machines. The references for the LU and QR decompositions are cited
   as [1] and [2] respectively.</p>

<h2 style="color:white">
<a id="Goals and Deliverables" class="anchor" href="#Goals-and-Deliverables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Goals and Deliverables</h2>

<p>The goal of the project is to develop a fast library in CUDA to implement atleast two matrix factorization
   routines.</p>
<p>   
   We expect to deliver an efficient, highly optimized implementation of the two routines on an NVIDIA GPU.
   Moreover, if time permits, we would also extend our project to include a similar efficient implementation of 
   Cholesky decomposition. Matrix multiplication is another application we would like to implement as a part of the
   library. Though not a decomposition routine, we believe it would be a good challenge to implement a optimized 
   matrix multiplication routine as near in performance as possible to state-of-the-art implementations. The 
   performance of the library will be measured by plotting speedup graphs and comparing them with state-of-the-art
   libraries. Another extension is to provide a R interface to these optimized CUDA routines, which was the original
   inspiration behind the project - making R code faster by GPU acceleration.
</p>

<h2 style="color:white">
<a id="Platform Choice" class="anchor" href="#Platform-Choice" aria-hidden="true"><span class="octicon octicon-link"></span></a>Platform Choice</h2>

<p>GPUs are suitable for matrix operations since they offer a huge amount of data parallelism.</p>

<h2 style="color:white">
<a id="Schedule" class="anchor" href="#Schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>Original Schedule</h2>

<table style="width:100%">
  <tr>
    <th>Day</th>
    <th>Planned Work</th>		
  </tr>
  <tr>
    <td>April 6</td>
    <td>Finalize proposal, Research efficient implementations </td>		
  </tr>
  <tr>
    <td>April 13</td>
    <td>Optimized LU Factorization routine</td>		
  </tr>
  <tr>
    <td>April 20</td>
    <td>Basic QR Factorization routine </td>		
  </tr>
  <tr>
    <td>April 27</td>
    <td>Optimize QR Factorization </td>		
  </tr>
  <tr>
    <td>May 4</td>
    <td>Cholesky/Matrix Multiply </td>		
  </tr>
  <tr>
    <td>May 11</td>
    <td>Finish final report and prepare presentation </td>		
  </tr>
</table>

<h2 style="color:white">
<a id="Schedule" class="anchor" href="#Schedule" aria-hidden="true"><span class="octicon octicon-link"></span></a>New Schedule</h2>

<table style="width:100%">
  <tr>
    <th>Day</th>
    <th>Planned Work</th>		
  </tr>
  <tr>
    <td>April 6</td>
    <td>Finalize proposal, Research efficient implementations, Sequential LU </td>		
  </tr>
  <tr>
    <td>April 13</td>
    <td>3 approaches for LU Factorization, some bugs with shared memory</td>		
  </tr>
  <tr>
    <td>April 20</td>
    <td>Sequential QR Factorization routine and naive CUDA implementation</td>		
  </tr>
  <tr>
    <td>April 24</td>
    <td>Optimize Matrix Multiply, Fix shared memory LU (Ravi) </td>		
  </tr>
  <tr>
    <td>April 27</td>
    <td>QR Factorization - optimal allocations (Harsha) </td>		
  </tr>
  <tr>
    <td>May 1</td>
    <td>Optimize QR - shared memory </td>		
  </tr>
  <tr>
    <td>May 4</td>
    <td>Benchmark results and Cholesky decomposition </td>		
  </tr>
  <tr>
    <td>May 7</td>
    <td>Optimize Cholesky and finish any pending work </td>		
  </tr>
  <tr>
    <td>May 11</td>
    <td>Finish final report and prepare presentation </td>		
  </tr>
</table>

<h3>
<a id="References" class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h3>

 <ol type="1">
  <li> Bandara, H. M. D. M., and D. N. Ranasinghe. "Effective GPU Strategies for LU Decomposition". Technical report, University of Colombo, Colombo, Sri Lanka, 2010.</li>
  <li>Kerr, Andrew, Dan Campbell, and Mark Richards. "QR decomposition on GPUs." Proceedings of 2nd Workshop on General Purpose Processing on Graphics Processing Units. ACM, 2009.</li>
</ol> 



